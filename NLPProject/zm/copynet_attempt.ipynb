{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"copynet_attempt.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2X6-QYZ1QiR2","executionInfo":{"status":"ok","timestamp":1619723087429,"user_tz":-180,"elapsed":37585,"user":{"displayName":"Danna Rosenfarb","photoUrl":"","userId":"02695992407678608014"}},"outputId":"bc319b5e-adfa-4078-a93a-cae118e13db7"},"source":["from google.colab import drive \n","drive.mount(\"/content/drive\")\n","DRIVE_PATH = \"/content/drive/MyDrive/NLPProject\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hv1hmLB8NVY_","executionInfo":{"status":"ok","timestamp":1619723114962,"user_tz":-180,"elapsed":6003,"user":{"displayName":"Danna Rosenfarb","photoUrl":"","userId":"02695992407678608014"}},"outputId":"4348d54e-8e58-4e79-cbef-a2a04917d703"},"source":["! git clone https://github.com/adamklec/copynet.git\n","! pip install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'copynet'...\n","remote: Enumerating objects: 30, done.\u001b[K\n","remote: Total 30 (delta 0), reused 0 (delta 0), pack-reused 30\u001b[K\n","Unpacking objects: 100% (30/30), done.\n","Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.0.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPD2OpJjNvAI","executionInfo":{"status":"ok","timestamp":1619723131890,"user_tz":-180,"elapsed":688,"user":{"displayName":"Danna Rosenfarb","photoUrl":"","userId":"02695992407678608014"}},"outputId":"27916648-089e-4f2e-9f02-84ab47496978"},"source":["import os \n","print(os.getcwd())\n","os.chdir(\"/content/copynet/\")\n","print(os.getcwd())\n","if not os.path.exists(\"/content/copynet/data\"):\n","    os.makedirs(\"/content/copynet/data\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","/content/copynet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xJNgDCuMNk0B"},"source":["import argparse\n","import time\n","import numpy as np\n","import pandas as pd\n","import torch\n","import string\n","from torch import optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, Dataset\n","import re\n","from dataset import SequencePairDataset\n","from model.encoder_decoder import EncoderDecoder\n","from evaluate import evaluate\n","from utils import to_np, trim_seqs\n","\n","from tensorboardX import SummaryWriter\n","from tqdm import tqdm\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhrCh-rlNZMU"},"source":["def train(encoder_decoder: EncoderDecoder,\n","          train_data_loader: DataLoader,\n","          model_name,\n","          val_data_loader: DataLoader,\n","          keep_prob,\n","          teacher_forcing_schedule,\n","          lr,\n","          max_length):\n","\n","    global_step = 0\n","    loss_function = torch.nn.NLLLoss(ignore_index=0)\n","    optimizer = optim.Adam(encoder_decoder.parameters(), lr=lr)\n","    model_path = './model/' + model_name + '/'\n","\n","    for epoch, teacher_forcing in enumerate(teacher_forcing_schedule):\n","        print('epoch %i' % epoch, flush=True)\n","\n","        for batch_idx, (input_idxs, target_idxs, input_tokens, target_tokens) in enumerate(tqdm(train_data_loader)):\n","            # input_idxs and target_idxs have dim (batch_size x max_len)\n","            # they are NOT sorted by length\n","\n","            lengths = (input_idxs != 0).long().sum(dim=1)\n","            sorted_lengths, order = torch.sort(lengths, descending=True)\n","\n","            input_variable = Variable(input_idxs[order, :][:, :max(lengths)])\n","            target_variable = Variable(target_idxs[order, :])\n","\n","            optimizer.zero_grad()\n","            output_log_probs, output_seqs = encoder_decoder(input_variable,\n","                                                            list(sorted_lengths),\n","                                                            targets=target_variable,\n","                                                            keep_prob=keep_prob,\n","                                                            teacher_forcing=teacher_forcing)\n","\n","            batch_size = input_variable.shape[0]\n","\n","            flattened_outputs = output_log_probs.view(batch_size * max_length, -1)\n","\n","            batch_loss = loss_function(flattened_outputs, target_variable.contiguous().view(-1))\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","            batch_outputs = trim_seqs(output_seqs)\n","\n","            batch_targets = [[list(seq[seq > 0])] for seq in list(to_np(target_variable))]\n","\n","            batch_bleu_score = corpus_bleu(batch_targets, batch_outputs, smoothing_function=SmoothingFunction().method1)\n","\n","            if global_step < 10 or (global_step % 10 == 0 and global_step < 100) or (global_step % 100 == 0 and epoch < 2):\n","                input_string = \"Amy, Please schedule a meeting with Marcos on Tuesday April 3rd. Adam Kleczewski\"\n","                output_string = encoder_decoder.get_response(input_string)\n","                writer.add_text('schedule', output_string, global_step=global_step)\n","\n","                input_string = \"Amy, Please cancel this meeting. Adam Kleczewski\"\n","                output_string = encoder_decoder.get_response(input_string)\n","                writer.add_text('cancel', output_string, global_step=global_step)\n","\n","            if global_step % 100 == 0:\n","\n","                writer.add_scalar('train_batch_loss', batch_loss, global_step)\n","                writer.add_scalar('train_batch_bleu_score', batch_bleu_score, global_step)\n","\n","                for tag, value in encoder_decoder.named_parameters():\n","                    tag = tag.replace('.', '/')\n","                    writer.add_histogram('weights/' + tag, value, global_step, bins='doane')\n","                    writer.add_histogram('grads/' + tag, to_np(value.grad), global_step, bins='doane')\n","\n","            global_step += 1\n","\n","        val_loss, val_bleu_score = evaluate(encoder_decoder, val_data_loader)\n","\n","        writer.add_scalar('val_loss', val_loss, global_step=global_step)\n","        writer.add_scalar('val_bleu_score', val_bleu_score, global_step=global_step)\n","\n","        encoder_embeddings = encoder_decoder.encoder.embedding.weight.data\n","        encoder_vocab = encoder_decoder.lang.tok_to_idx.keys()\n","        writer.add_embedding(encoder_embeddings, metadata=encoder_vocab, global_step=0, tag='encoder_embeddings')\n","\n","        decoder_embeddings = encoder_decoder.decoder.embedding.weight.data\n","        decoder_vocab = encoder_decoder.lang.tok_to_idx.keys()\n","        writer.add_embedding(decoder_embeddings, metadata=decoder_vocab, global_step=0, tag='decoder_embeddings')\n","\n","        input_string = \"Amy, Please schedule a meeting with Marcos on Tuesday April 3rd. Adam Kleczewski\"\n","        output_string = encoder_decoder.get_response(input_string)\n","        writer.add_text('schedule', output_string, global_step=global_step)\n","\n","        input_string = \"Amy, Please cancel this meeting. Adam Kleczewski\"\n","        output_string = encoder_decoder.get_response(input_string)\n","        writer.add_text('cancel', output_string, global_step=global_step)\n","\n","        print('val loss: %.5f, val BLEU score: %.5f' % (val_loss, val_bleu_score), flush=True)\n","        torch.save(encoder_decoder, \"%s%s_%i.pt\" % (model_path, model_name, epoch))\n","\n","        print('-' * 100, flush=True)\n","\n","\n","def main(model_name, use_cuda, batch_size, teacher_forcing_schedule, keep_prob, val_size, lr, decoder_type, vocab_limit, hidden_size, embedding_size, max_length, seed=42):\n","\n","    model_path = './model/' + model_name + '/'\n","\n","    # TODO: Change logging to reflect loaded parameters\n","\n","    print(\"training %s with use_cuda=%s, batch_size=%i\"% (model_name, use_cuda, batch_size), flush=True)\n","    print(\"teacher_forcing_schedule=\", teacher_forcing_schedule, flush=True)\n","    print(\"keep_prob=%f, val_size=%f, lr=%f, decoder_type=%s, vocab_limit=%i, hidden_size=%i, embedding_size=%i, max_length=%i, seed=%i\" % (keep_prob, val_size, lr, decoder_type, vocab_limit, hidden_size, embedding_size, max_length, seed), flush=True)\n","\n","    if os.path.isdir(model_path):\n","\n","        print(\"loading encoder and decoder from model_path\", flush=True)\n","        encoder_decoder = torch.load(model_path + model_name + '.pt')\n","\n","        print(\"creating training and validation datasets with saved languages\", flush=True)\n","        train_dataset = SequencePairDataset(lang=encoder_decoder.lang,\n","                                            use_cuda=use_cuda,\n","                                            is_val=False,\n","                                            val_size=val_size,\n","                                            use_extended_vocab=(encoder_decoder.decoder_type=='copy'))\n","\n","        val_dataset = SequencePairDataset(lang=encoder_decoder.lang,\n","                                          use_cuda=use_cuda,\n","                                          is_val=True,\n","                                          val_size=val_size,\n","                                          use_extended_vocab=(encoder_decoder.decoder_type=='copy'))\n","\n","    else:\n","        os.mkdir(model_path)\n","\n","        print(\"creating training and validation datasets\", flush=True)\n","        train_dataset = SequencePairDataset(vocab_limit=vocab_limit,\n","                                            use_cuda=use_cuda,\n","                                            is_val=False,\n","                                            val_size=val_size,\n","                                            seed=seed,\n","                                            use_extended_vocab=(decoder_type=='copy'))\n","\n","        val_dataset = SequencePairDataset(lang=train_dataset.lang,\n","                                          use_cuda=use_cuda,\n","                                          is_val=True,\n","                                          val_size=val_size,\n","                                          seed=seed,\n","                                          use_extended_vocab=(decoder_type=='copy'))\n","\n","        print(\"creating encoder-decoder model\", flush=True)\n","        encoder_decoder = EncoderDecoder(train_dataset.lang,\n","                                         max_length,\n","                                         embedding_size,\n","                                         hidden_size,\n","                                         decoder_type)\n","\n","        torch.save(encoder_decoder, model_path + '/%s.pt' % model_name)\n","\n","    if use_cuda:\n","        encoder_decoder = encoder_decoder.cuda()\n","    else:\n","        encoder_decoder = encoder_decoder.cpu()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZfosIRKOBGy","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1619723413895,"user_tz":-180,"elapsed":709,"user":{"displayName":"Danna Rosenfarb","photoUrl":"","userId":"02695992407678608014"}},"outputId":"f8e4684e-2997-4094-ea7e-ca0a7072814e"},"source":["# NEED TO UPLOAD THIS!!\n","df = pd.read_pickle(\"/content/data.pkl\")\n","\n","def remove_notes(word_list):\n","  return \" \".join([re.sub(r'[\\u0591-\\u05BD\\u05BF-\\u05C2\\u05C4-\\u05C7]', '', token) for token in word_list])\n","\n","df[\"input_sentence\"] = df[\"input_sentence\"].apply(lambda row: remove_notes(row))\n","\n","df[\"target_sentences\"] = df[\"target_sentences\"].apply(lambda row: remove_notes(row))\n","results = ['א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'ך',  'כ',  'ל',  'ם',  'מ','ן', 'נ','ס', 'ע', 'ף', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת']\n","map_letters = dict(list(zip(results, string.ascii_lowercase + \"@\")))\n","df = df.replace(map_letters, regex=True)\n","\n","df[\"input_sentence_reveresed\"] = df[\"input_sentence\"].apply(lambda row: row[::-1])\n","df[\"target_sentences_reveresed\"] = df[\"target_sentences\"].apply(lambda row: row[::-1])\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-b7f73a8e97c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NEED TO UPLOAD THIS!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\u0591-\\u05BD\\u05BF-\\u05C2\\u05C4-\\u05C7]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data.pkl'"]}]},{"cell_type":"code","metadata":{"id":"C-2bmiYkMV4M"},"source":["# dash to spaces "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUPZeHolL_5J","executionInfo":{"status":"ok","timestamp":1619541861951,"user_tz":240,"elapsed":289,"user":{"displayName":"Zachary Metzman","photoUrl":"","userId":"12527305156258230556"}},"outputId":"df97369d-e1f3-41df-e9d1-2309a3e5ba94"},"source":["df[\"input_sentence\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0           fjmbzn sfy l@qf@ fmaz@f madn amejn jefe fjsz\n","1               oaf@ faybs amt fhozjn zmze fuxdjen fwbaf\n","2         szf lp a@־oze jefe wfe azy llm jzyam bqj fjszf\n","3      wba jwa lm fosme zqe szyjn obp zo@ boruy ab@n ...\n","4      mamejqf qgbhe qmle maoy wsxjn en sm־lp en lj־q...\n","                             ...                        \n","125                   a@־sby jmd fzmh a@־zmh jmd fayulzd\n","126     ysef zu@ ajz jzosf ma azy zu@n zn fqbme qyde ebe\n","127    op־ejay ojn mz@f@ owyjn fqmaf ejay fbaz @of@ a...\n","128    bp־sojqdb qhzfp xybp ge hoze bqj־zqe lbzjn hoz...\n","129    jsxb famej jwhx amej abyen amej ab@n amej jefe...\n","Name: input_sentence, Length: 1172, dtype: object"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"xmoTaF5YSlnX"},"source":["# res = []\n","# for sentence in df.input_sentence.values:\n","#   for token in sentence.split(\" \"):\n","#     for char in token:\n","#       res.append(char)\n","\n","# charcters = set(res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cximxwb9U2Vl","executionInfo":{"status":"ok","timestamp":1619541863747,"user_tz":240,"elapsed":185,"user":{"displayName":"Zachary Metzman","photoUrl":"","userId":"12527305156258230556"}},"outputId":"7aaf2cf6-510e-4818-b153-286e08783f5b"},"source":["df[\"data_type\"].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['test', 'val'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"rZhGsTX7U-Nn"},"source":["vals = {\"test\":0, \"val\":0}\n","train_val_test = {\"test\":[], \"val\":[]}\n","for idx, (i, row) in enumerate(df.iterrows()): # copy path if copy\n","    # print(row[\"data_type\"])\n","    path = \"/content/copynet/data/{}{}.txt\".format(row[\"data_type\"], vals[row[\"data_type\"]])\n","    row[[\"input_sentence_reveresed\",\t\"target_sentences_reveresed\"]].to_csv(path, index=False, header=False)\n","    train_val_test[row[\"data_type\"]].append(path)\n","    vals[row[\"data_type\"]] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYx7wBuIVo20"},"source":["class CustomDataSet(Dataset):\n","  \"\"\"\n","  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n","  \"\"\"\n","  def __init__(self, file_name_list, transform=None):\n","    self.file_name_list = file_name_list\n","    self.transform = transform\n","  \n","  def __len__(self):\n","      # Return the length of the dataset\n","    return len(self.file_name_list)\n","  \n","  def __getitem__(self, idx):\n","      # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n","    if torch.is_tensor(idx):\n","        idx = int(idx.item())\n","    file_path = self.file_name_list[idx]\n","    input_val, target_val = pd.read_csv(file_path, header=None).T.values.ravel()\n","    return (input_val, target_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"jboQ3IQ_XkQ6","executionInfo":{"status":"error","timestamp":1619541869798,"user_tz":240,"elapsed":172,"user":{"displayName":"Zachary Metzman","photoUrl":"","userId":"12527305156258230556"}},"outputId":"3d31aa9b-38e0-4b70-9eb9-8d5f2286c2d9"},"source":["# train_data_loader = dataloaders_dict[\"test\"]\n","# val_data_loader = dataloaders_dict[\"val\"]\n","\n","if True:\n","    encoder_decoder = encoder_decoder.cuda()\n","else:\n","    encoder_decoder = encoder_decoder.cpu()\n","\n","\n","\n","train(encoder_decoder,\n","      train_data_loader,\n","      model_name,\n","      val_data_loader,\n","      keep_prob,\n","      teacher_forcing_schedule,\n","      lr,\n","      encoder_decoder.decoder.max_length)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-56106d388a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mencoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mencoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'encoder_decoder' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSPbBAx-YrVi","executionInfo":{"status":"ok","timestamp":1619393690676,"user_tz":240,"elapsed":405,"user":{"displayName":"Zachary Metzman","photoUrl":"","userId":"12527305156258230556"}},"outputId":"2a708d23-8db2-4cdb-fc8c-915a074b9743"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'test': <torch.utils.data.dataloader.DataLoader at 0x7f274a595390>,\n"," 'val': <torch.utils.data.dataloader.DataLoader at 0x7f274a371390>}"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"tvERkiJhVuo9"},"source":["    print(image_datasets)\n","    # Create training and validation dataloaders\n","    # Never shuffle the test set\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"DjdBNPAcOLIv","executionInfo":{"status":"error","timestamp":1619326594096,"user_tz":240,"elapsed":241,"user":{"displayName":"Zachary Metzman","photoUrl":"","userId":"12527305156258230556"}},"outputId":"923e95da-0938-4c43-87da-14496594ee34"},"source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Parse training parameters')\n","    parser.add_argument('./data/', type=str,\n","                        help='the name of a subdirectory of ./model/ that '\n","                             'contains encoder and decoder model files')\n","    \n","    parser.add_argument('--epochs', type=int, default=10,\n","                        help='the number of epochs to train')\n","\n","    parser.add_argument('--use_cuda', action='store_true',\n","                        help='flag indicating that cuda will be used')\n","\n","    parser.add_argument('--batch_size', type=int, default=128,\n","                        help='number of examples in a batch')\n","\n","    parser.add_argument('--teacher_forcing_fraction', type=float, default=0.5,\n","                        help='fraction of batches that will use teacher forcing during training')\n","\n","    parser.add_argument('--scheduled_teacher_forcing', action='store_true',\n","                        help='Linearly decrease the teacher forcing fraction '\n","                             'from 1.0 to 0.0 over the specified number of epocs')\n","\n","    parser.add_argument('--keep_prob', type=float, default=1.0,\n","                        help='Probablity of keeping an element in the dropout step.')\n","\n","    parser.add_argument('--val_size', type=float, default=0.1,\n","                        help='fraction of data to use for validation')\n","\n","    parser.add_argument('--lr', type=float, default=0.001,\n","                        help='Learning rate.')\n","\n","    parser.add_argument('--decoder_type', type=str, default='copy',\n","                        help=\"Allowed values 'copy' or 'attn'\")\n","\n","    parser.add_argument('--vocab_limit', type=int, default=5000,\n","                        help='When creating a new Language object the vocab'\n","                             'will be truncated to the most frequently'\n","                             'occurring words in the training dataset.')\n","\n","    parser.add_argument('--hidden_size', type=int, default=256,\n","                        help='The number of RNN units in the encoder. 2x this '\n","                             'number of RNN units will be used in the decoder')\n","\n","    parser.add_argument('--embedding_size', type=int, default=128,\n","                        help='Embedding size used in both encoder and decoder')\n","\n","    parser.add_argument('--max_length', type=int, default=200,\n","                        help='Sequences will be padded or truncated to this size.')\n","\n","    args = parser.parse_args()\n","\n","    writer = SummaryWriter('./logs/%s_%s' % (args.model_name, str(int(time.time()))))\n","    if args.scheduled_teacher_forcing:\n","        schedule = np.arange(1.0, 0.0, -1.0/args.epochs)\n","    else:\n","        schedule = np.ones(args.epochs) * args.teacher_forcing_fraction\n","\n","    main(args.model_name, args.use_cuda, args.batch_size, schedule, args.keep_prob, args.val_size, args.lr, args.decoder_type, args.vocab_limit, args.hidden_size, args.embedding_size, args.max_length)\n","    # main(str(int(time.time())), args.use_cuda, args.batch_size, schedule, args.keep_prob, args.val_size, args.lr, args.decoder_type, args.vocab_limit, args.hidden_size, args.embedding_size, args.max_length)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--use_cuda]\n","                             [--batch_size BATCH_SIZE]\n","                             [--teacher_forcing_fraction TEACHER_FORCING_FRACTION]\n","                             [--scheduled_teacher_forcing]\n","                             [--keep_prob KEEP_PROB] [--val_size VAL_SIZE]\n","                             [--lr LR] [--decoder_type DECODER_TYPE]\n","                             [--vocab_limit VOCAB_LIMIT]\n","                             [--hidden_size HIDDEN_SIZE]\n","                             [--embedding_size EMBEDDING_SIZE]\n","                             [--max_length MAX_LENGTH]\n","                             ./data/\n","ipykernel_launcher.py: error: unrecognized arguments: -f\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Pz8ZDv1EYzID"},"source":["\n","dataloaders_dict = {x: torch.utils.data.DataLoader(train_val_test[x], batch_size=128, shuffle=False if x != 'train' else shuffle, num_workers=1) for x in [\"test\", \"val\"]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"AzI15WoLRt_e","executionInfo":{"status":"error","timestamp":1619326890837,"user_tz":240,"elapsed":755,"user":{"displayName":"Zachary Metzman","photoUrl":"","userId":"12527305156258230556"}},"outputId":"fc2015ef-4edd-437f-f1a5-a7ab04d8bb35"},"source":["main('./model/', True, 128, None, 1, 0.1, 0.001, 'copy', 5000, 256, 128, 200)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training ./model/ with use_cuda=True, batch_size=128\n","teacher_forcing_schedule= None\n","keep_prob=1.000000, val_size=0.100000, lr=0.001000, decoder_type=copy, vocab_limit=5000, hidden_size=256, embedding_size=128, max_length=200, seed=42\n","creating training and validation datasets\n","reading file 0/1055\n","reading file 1000/1055\n","creating encoder-decoder model\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-925715757009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'copy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-43-66caf7b08ca5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_name, use_cuda, batch_size, teacher_forcing_schedule, keep_prob, val_size, lr, decoder_type, vocab_limit, hidden_size, embedding_size, max_length, seed)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                          decoder_type)\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/%s.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/./model///./model/.pt'"]}]}]}